{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashishpapanai/miniconda3/envs/timm/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import (\n",
    "    CenterCrop,\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomResizedCrop,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    ")\n",
    "import torch\n",
    "\n",
    "ImageNet_mean = [0.485, 0.456, 0.406]\n",
    "ImageNet_std = [0.229, 0.224, 0.225]\n",
    "# ImageNet_mean = [0.5, 0.5, 0.5]\n",
    "# ImageNet_std = [0.5, 0.5, 0.5]\n",
    "normalize = Normalize(mean=ImageNet_mean, std=ImageNet_std)\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        RandomResizedCrop(224),\n",
    "        RandomHorizontalFlip(),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        # Resize(image_processor.size[\"height\"]),\n",
    "        Resize(224),\n",
    "        CenterCrop(224),\n",
    "        ToTensor(),\n",
    "        normalize,\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [train_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch\n",
    "\n",
    "\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    example_batch[\"pixel_values\"] = [val_transforms(image.convert(\"RGB\")) for image in example_batch[\"image\"]]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loaders.ImageData import ImageDataset\n",
    "data_dir = \"/bucket/npss/CottonPestClassification_v3a_npss/\"\n",
    "\n",
    "train_loader = ImageDataset(data_dir, split='train', transform=train_transforms)\n",
    "val_loader = ImageDataset(data_dir, split='val', transform=val_transforms)\n",
    "test_loader = ImageDataset(data_dir, split='reporting', transform=val_transforms)\n",
    "holdout_loader = ImageDataset(data_dir, split='holdout', transform=val_transforms)\n",
    "# val_loader = ImageDataset(data_dir, split='val')\n",
    "# test_loader = ImageDataset(data_dir, split='test')\n",
    "# holdout_loader = ImageDataset(data_dir, split='holdout')\n",
    "\n",
    "label2id = train_loader.labels2id\n",
    "id2label = train_loader.id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /bucket/experiments_ashish/vit-base-patch16-224-in21k-finetune-os100/checkpoint-911/ were not used when initializing ViTForImageClassification: ['classifier.modules_to_save.default.bias', 'classifier.modules_to_save.default.weight', 'classifier.original_module.bias', 'classifier.original_module.weight', 'vit.encoder.layer.0.attention.attention.query.base_layer.bias', 'vit.encoder.layer.0.attention.attention.query.base_layer.weight', 'vit.encoder.layer.0.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.0.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.0.attention.attention.value.base_layer.bias', 'vit.encoder.layer.0.attention.attention.value.base_layer.weight', 'vit.encoder.layer.0.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.0.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.1.attention.attention.query.base_layer.bias', 'vit.encoder.layer.1.attention.attention.query.base_layer.weight', 'vit.encoder.layer.1.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.1.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.1.attention.attention.value.base_layer.bias', 'vit.encoder.layer.1.attention.attention.value.base_layer.weight', 'vit.encoder.layer.1.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.1.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.10.attention.attention.query.base_layer.bias', 'vit.encoder.layer.10.attention.attention.query.base_layer.weight', 'vit.encoder.layer.10.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.10.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.10.attention.attention.value.base_layer.bias', 'vit.encoder.layer.10.attention.attention.value.base_layer.weight', 'vit.encoder.layer.10.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.10.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.11.attention.attention.query.base_layer.bias', 'vit.encoder.layer.11.attention.attention.query.base_layer.weight', 'vit.encoder.layer.11.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.11.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.11.attention.attention.value.base_layer.bias', 'vit.encoder.layer.11.attention.attention.value.base_layer.weight', 'vit.encoder.layer.11.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.11.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.2.attention.attention.query.base_layer.bias', 'vit.encoder.layer.2.attention.attention.query.base_layer.weight', 'vit.encoder.layer.2.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.2.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.2.attention.attention.value.base_layer.bias', 'vit.encoder.layer.2.attention.attention.value.base_layer.weight', 'vit.encoder.layer.2.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.2.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.3.attention.attention.query.base_layer.bias', 'vit.encoder.layer.3.attention.attention.query.base_layer.weight', 'vit.encoder.layer.3.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.3.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.3.attention.attention.value.base_layer.bias', 'vit.encoder.layer.3.attention.attention.value.base_layer.weight', 'vit.encoder.layer.3.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.3.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.4.attention.attention.query.base_layer.bias', 'vit.encoder.layer.4.attention.attention.query.base_layer.weight', 'vit.encoder.layer.4.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.4.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.4.attention.attention.value.base_layer.bias', 'vit.encoder.layer.4.attention.attention.value.base_layer.weight', 'vit.encoder.layer.4.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.4.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.5.attention.attention.query.base_layer.bias', 'vit.encoder.layer.5.attention.attention.query.base_layer.weight', 'vit.encoder.layer.5.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.5.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.5.attention.attention.value.base_layer.bias', 'vit.encoder.layer.5.attention.attention.value.base_layer.weight', 'vit.encoder.layer.5.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.5.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.6.attention.attention.query.base_layer.bias', 'vit.encoder.layer.6.attention.attention.query.base_layer.weight', 'vit.encoder.layer.6.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.6.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.6.attention.attention.value.base_layer.bias', 'vit.encoder.layer.6.attention.attention.value.base_layer.weight', 'vit.encoder.layer.6.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.6.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.7.attention.attention.query.base_layer.bias', 'vit.encoder.layer.7.attention.attention.query.base_layer.weight', 'vit.encoder.layer.7.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.7.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.7.attention.attention.value.base_layer.bias', 'vit.encoder.layer.7.attention.attention.value.base_layer.weight', 'vit.encoder.layer.7.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.7.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.8.attention.attention.query.base_layer.bias', 'vit.encoder.layer.8.attention.attention.query.base_layer.weight', 'vit.encoder.layer.8.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.8.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.8.attention.attention.value.base_layer.bias', 'vit.encoder.layer.8.attention.attention.value.base_layer.weight', 'vit.encoder.layer.8.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.8.attention.attention.value.lora_B.default.weight', 'vit.encoder.layer.9.attention.attention.query.base_layer.bias', 'vit.encoder.layer.9.attention.attention.query.base_layer.weight', 'vit.encoder.layer.9.attention.attention.query.lora_A.default.weight', 'vit.encoder.layer.9.attention.attention.query.lora_B.default.weight', 'vit.encoder.layer.9.attention.attention.value.base_layer.bias', 'vit.encoder.layer.9.attention.attention.value.base_layer.weight', 'vit.encoder.layer.9.attention.attention.value.lora_A.default.weight', 'vit.encoder.layer.9.attention.attention.value.lora_B.default.weight']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /bucket/experiments_ashish/vit-base-patch16-224-in21k-finetune-os100/checkpoint-911/ and are newly initialized: ['classifier.bias', 'classifier.weight', 'vit.encoder.layer.0.attention.attention.query.bias', 'vit.encoder.layer.0.attention.attention.query.weight', 'vit.encoder.layer.0.attention.attention.value.bias', 'vit.encoder.layer.0.attention.attention.value.weight', 'vit.encoder.layer.1.attention.attention.query.bias', 'vit.encoder.layer.1.attention.attention.query.weight', 'vit.encoder.layer.1.attention.attention.value.bias', 'vit.encoder.layer.1.attention.attention.value.weight', 'vit.encoder.layer.10.attention.attention.query.bias', 'vit.encoder.layer.10.attention.attention.query.weight', 'vit.encoder.layer.10.attention.attention.value.bias', 'vit.encoder.layer.10.attention.attention.value.weight', 'vit.encoder.layer.11.attention.attention.query.bias', 'vit.encoder.layer.11.attention.attention.query.weight', 'vit.encoder.layer.11.attention.attention.value.bias', 'vit.encoder.layer.11.attention.attention.value.weight', 'vit.encoder.layer.2.attention.attention.query.bias', 'vit.encoder.layer.2.attention.attention.query.weight', 'vit.encoder.layer.2.attention.attention.value.bias', 'vit.encoder.layer.2.attention.attention.value.weight', 'vit.encoder.layer.3.attention.attention.query.bias', 'vit.encoder.layer.3.attention.attention.query.weight', 'vit.encoder.layer.3.attention.attention.value.bias', 'vit.encoder.layer.3.attention.attention.value.weight', 'vit.encoder.layer.4.attention.attention.query.bias', 'vit.encoder.layer.4.attention.attention.query.weight', 'vit.encoder.layer.4.attention.attention.value.bias', 'vit.encoder.layer.4.attention.attention.value.weight', 'vit.encoder.layer.5.attention.attention.query.bias', 'vit.encoder.layer.5.attention.attention.query.weight', 'vit.encoder.layer.5.attention.attention.value.bias', 'vit.encoder.layer.5.attention.attention.value.weight', 'vit.encoder.layer.6.attention.attention.query.bias', 'vit.encoder.layer.6.attention.attention.query.weight', 'vit.encoder.layer.6.attention.attention.value.bias', 'vit.encoder.layer.6.attention.attention.value.weight', 'vit.encoder.layer.7.attention.attention.query.bias', 'vit.encoder.layer.7.attention.attention.query.weight', 'vit.encoder.layer.7.attention.attention.value.bias', 'vit.encoder.layer.7.attention.attention.value.weight', 'vit.encoder.layer.8.attention.attention.query.bias', 'vit.encoder.layer.8.attention.attention.query.weight', 'vit.encoder.layer.8.attention.attention.value.bias', 'vit.encoder.layer.8.attention.attention.value.weight', 'vit.encoder.layer.9.attention.attention.query.bias', 'vit.encoder.layer.9.attention.attention.query.weight', 'vit.encoder.layer.9.attention.attention.value.bias', 'vit.encoder.layer.9.attention.attention.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#load lora model from local\n",
    "from transformers import AutoModelForImageClassification\n",
    "from peft import LoraConfig, get_peft_model, PeftModel\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"query\", \"value\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    modules_to_save=[\"classifier\"],\n",
    ")\n",
    "\n",
    "model_checkpoint = \"/bucket/experiments_ashish/vit-base-patch16-224-in21k-finetune-os100/checkpoint-911/\"\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")\n",
    "\n",
    "# inference_model = PeftModel.from_pretrained(\n",
    "#                                     model,\n",
    "#                                     'ashishp-wiai/vit-base-patch16-224-in21k-finetuned-CottonPestClassification_v3a_os')\n",
    "inference_model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image_processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # image_processor \n",
    "# ImageNet_mean = [0.485, 0.456, 0.406]\n",
    "# ImageNet_std = [0.229, 0.224, 0.225]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outs = val_loader.__getitem__(0)\n",
    "# path = outs['image_file_path']\n",
    "# img = outs['image']\n",
    "# label = outs['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding = image_processor(img, return_tensors=\"pt\")\n",
    "# with torch.no_grad():\n",
    "#     # check = {'pixel_values': img}\n",
    "#     output = inference_model(**encoding)\n",
    "#     logits = output.logits\n",
    "#     probs = logits.softmax(dim=1)\n",
    "#     print(probs, probs.argmax(dim=1), label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding['pixel_values'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outs_df = pd.DataFrame(columns=['image_file_path', 'label', 'pred_label', 'pred_prob', 'logits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 1050/1413 [03:46<03:26,  1.75it/s]/home/ashishpapanai/miniconda3/envs/timm/lib/python3.10/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (108576768 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1413/1413 [06:06<00:00,  3.85it/s]\n"
     ]
    }
   ],
   "source": [
    "path_list, label_list, pred_label_list, pred_prob_list, logits_list = [], [], [], [], []\n",
    "for item in tqdm(val_loader):\n",
    "    path = item['image_file_path']\n",
    "    img = item['image']\n",
    "    label = item['labels']\n",
    "\n",
    "    # encoding = image_processor(img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    encoding_val = {'pixel_values': img.unsqueeze(0)}\n",
    "    with torch.no_grad():\n",
    "        output = inference_model(**encoding_val)\n",
    "        logits = output.logits\n",
    "        probs = logits.softmax(dim=1)\n",
    "    \n",
    "    path_list.append(path)\n",
    "    label_list.append(label)\n",
    "    pred_label_list.append(probs.argmax(dim=1).item())\n",
    "    pred_prob_list.append(probs)\n",
    "    logits_list.append(logits)\n",
    "\n",
    "val_outs_df['image_file_path'] = path_list\n",
    "val_outs_df['label'] = label_list\n",
    "val_outs_df['pred_label'] = pred_label_list\n",
    "val_outs_df['pred_prob'] = pred_prob_list\n",
    "val_outs_df['logits'] = logits_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 1341/1867 [05:02<04:07,  2.13it/s]/home/ashishpapanai/miniconda3/envs/timm/lib/python3.10/site-packages/PIL/Image.py:3186: DecompressionBombWarning: Image size (108576768 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.\n",
      "  warnings.warn(\n",
      "100%|██████████| 1867/1867 [08:36<00:00,  3.61it/s]\n"
     ]
    }
   ],
   "source": [
    "test_outs_df = pd.DataFrame(columns=['image_file_path', 'label', 'pred_label', 'pred_prob', 'logits'])\n",
    "path_list_test, label_list_test, pred_label_list_test, pred_prob_list_test, logits_list_test = [], [], [], [], []\n",
    "for item in tqdm(test_loader):\n",
    "    path = item['image_file_path']\n",
    "    img = item['image']\n",
    "    label = item['labels']\n",
    "\n",
    "    # encoding = image_processor(img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    encoding_test = {'pixel_values': img.unsqueeze(0)}    \n",
    "    # print(img.shape, encoding['pixel_values'].shape)\n",
    "    with torch.no_grad():\n",
    "        output = inference_model(**encoding_test)\n",
    "        logits = output.logits\n",
    "        probs = logits.softmax(dim=1)\n",
    "    \n",
    "    path_list_test.append(path)\n",
    "    label_list_test.append(label)\n",
    "    pred_label_list_test.append(probs.argmax(dim=1))\n",
    "    pred_prob_list_test.append(probs)\n",
    "    logits_list_test.append(logits)\n",
    "\n",
    "test_outs_df['image_file_path'] = path_list_test\n",
    "test_outs_df['label'] = label_list_test\n",
    "test_outs_df['pred_label'] = pred_label_list_test\n",
    "test_outs_df['pred_prob'] = pred_prob_list_test\n",
    "test_outs_df['logits'] = logits_list_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 101/101 [00:19<00:00,  5.29it/s]\n"
     ]
    }
   ],
   "source": [
    "npss_outs_df = pd.DataFrame(columns=['image_file_path', 'label', 'pred_label', 'pred_prob', 'logits'])\n",
    "path_list_npss, label_list_npss, pred_label_list_npss, pred_prob_list_npss, logits_list_npss = [], [], [], [], []\n",
    "for item in tqdm(holdout_loader):\n",
    "    path = item['image_file_path']\n",
    "    img = item['image']\n",
    "    label = item['labels']\n",
    "\n",
    "    # encoding = image_processor(img.convert(\"RGB\"), return_tensors=\"pt\")\n",
    "    encoding_hold = {'pixel_values': img.unsqueeze(0)}\n",
    "    with torch.no_grad():\n",
    "        output = inference_model(**encoding_hold)\n",
    "        logits = output.logits\n",
    "        probs = logits.softmax(dim=1)\n",
    "    \n",
    "    path_list_npss.append(path)\n",
    "    label_list_npss.append(label)\n",
    "    pred_label_list_npss.append(probs.argmax(dim=1))\n",
    "    pred_prob_list_npss.append(probs)\n",
    "    logits_list_npss.append(logits)\n",
    "\n",
    "npss_outs_df['image_file_path'] = path_list_npss\n",
    "npss_outs_df['label'] = label_list_npss\n",
    "npss_outs_df['pred_label'] = pred_label_list_npss\n",
    "npss_outs_df['pred_prob'] = pred_prob_list_npss\n",
    "npss_outs_df['logits'] = logits_list_npss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(label\n",
       " 1    928\n",
       " 2    452\n",
       " 0     33\n",
       " Name: count, dtype: int64,\n",
       " pred_label\n",
       " 0    1339\n",
       " 2      73\n",
       " 1       1\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_outs_df.label.value_counts(), val_outs_df.pred_label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_outs_df['pred_label'] = val_outs_df['pred_label'].apply(lambda x: x.item())\n",
    "test_outs_df['pred_label'] = test_outs_df['pred_label'].apply(lambda x: x.item())\n",
    "npss_outs_df['pred_label'] = npss_outs_df['pred_label'].apply(lambda x: x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ashishpapanai/miniconda3/envs/timm/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "val_acc = accuracy_score(val_outs_df['label'], val_outs_df['pred_label'])\n",
    "val_prec = precision_score(val_outs_df['label'], val_outs_df['pred_label'], average=None)\n",
    "val_recall = recall_score(val_outs_df['label'], val_outs_df['pred_label'], average=None)\n",
    "val_f1 = f1_score(val_outs_df['label'], val_outs_df['pred_label'], average=None)\n",
    "\n",
    "test_acc = accuracy_score(test_outs_df['label'], test_outs_df['pred_label'])\n",
    "test_prec = precision_score(test_outs_df['label'], test_outs_df['pred_label'], average=None)\n",
    "test_recall = recall_score(test_outs_df['label'], test_outs_df['pred_label'], average=None)\n",
    "test_f1 = f1_score(test_outs_df['label'], test_outs_df['pred_label'], average=None)\n",
    "\n",
    "npss_acc = accuracy_score(npss_outs_df['label'], npss_outs_df['pred_label'])\n",
    "npss_prec = precision_score(npss_outs_df['label'], npss_outs_df['pred_label'], average=None)\n",
    "npss_recall = recall_score(npss_outs_df['label'], npss_outs_df['pred_label'], average=None)\n",
    "npss_f1 = f1_score(npss_outs_df['label'], npss_outs_df['pred_label'], average=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapping:  {'aphids': 0, 'none': 1, 'whitefly': 2}\n",
      "Validation Accuracy:  0.0226\n",
      "Validation Precision:  [0.02315161 1.         0.        ]\n",
      "Validation Recall:  [0.93939394 0.00107759 0.        ]\n",
      "Validation F1:  [0.0451895  0.00215285 0.        ]\n",
      "Test Accuracy:  0.0498\n",
      "Test Precision:  [0.05025409 1.         0.0212766 ]\n",
      "Test Recall:  [0.98888889 0.00171821 0.00326264]\n",
      "Test F1:  [0.0956475  0.00343053 0.00565771]\n",
      "NPSS Accuracy:  0.505\n",
      "NPSS Precision:  [0.5 0.  1. ]\n",
      "NPSS Recall:  [1.   0.   0.02]\n",
      "NPSS F1:  [0.66666667 0.         0.03921569]\n"
     ]
    }
   ],
   "source": [
    "print(\"Mapping: \", label2id)\n",
    "print(\"Validation Accuracy: \", round(val_acc, 4))\n",
    "print(\"Validation Precision: \", val_prec)\n",
    "print(\"Validation Recall: \", val_recall)\n",
    "print(\"Validation F1: \", val_f1)\n",
    "\n",
    "print(\"Test Accuracy: \", round(test_acc, 4))\n",
    "print(\"Test Precision: \", test_prec)\n",
    "print(\"Test Recall: \", test_recall)\n",
    "print(\"Test F1: \", test_f1)\n",
    "\n",
    "print(\"NPSS Accuracy: \", round(npss_acc, 4))\n",
    "print(\"NPSS Precision: \", npss_prec)\n",
    "print(\"NPSS Recall: \", npss_recall)\n",
    "print(\"NPSS F1: \", npss_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_outs_df.to_csv('./output/Vit_FineTune_0304_val.csv', index=False)\n",
    "test_outs_df.to_csv('./output/Vit_FineTune_0304_test.csv', index=False)\n",
    "npss_outs_df.to_csv('./output/Vit_FineTune_0304_npss.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inference_model.push_to_hub('ashishp-wiai/vit-base-patch16-224-in21k-finetuned-CottonPestClassification_v3a_os')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "timm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
